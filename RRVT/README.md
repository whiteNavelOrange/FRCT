[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rvt-2-learning-precise-manipulation-from-few/robot-manipulation-on-rlbench)](https://paperswithcode.com/sota/robot-manipulation-on-rlbench?p=rvt-2-learning-precise-manipulation-from-few)





<!-- <div style="display:flex">
    <div style="flex:1;padding-right:5px;">
        <img src="https://robotic-view-transformer-2.github.io/figs/teaser.gif" alt="RVT-2" height="200px"/>
        <p style="text-align:center;">RVT-2 solving high precision tasks</p>
    </div>
    <div style="flex:1;padding-left:5px;">
        <img src="https://robotic-view-transformer.github.io/real_world/real_world_very_small.gif" alt="RVT" height="200px"/>
        <p style="text-align:center;">Single RVT model model solving multiple tasks</p>
    </div>
</div> -->



We extend [RVT](https://github.com/nvlabs/rvt)'s single-arm manipulation capabilities to bimanual tasks through a novel architecture (FRCT) that achieves arm coordination within a unified model.



